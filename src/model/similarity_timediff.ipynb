{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_classif,SelectPercentile\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "data_no_additional_feature = '../data/training_user_tweet.csv'\n",
    "data_with_tweet_csv = '../data/training_user_tweet.csv'\n",
    "#tweets_user='tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from math import factorial\n",
    "import itertools as it\n",
    "from numpy import linalg as LA\n",
    "\n",
    "#%%\n",
    "def modify_df(user_tweets):\n",
    "    \"\"\"\n",
    "    remove unnecessary words from the user_tweets.csv\n",
    "    \n",
    "    Argument: user_tweets\n",
    "    \n",
    "    Return: user_tweets dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    special_remove = [str(tweets) for tweets in user_tweets['text']] \n",
    "    user_tweets['tweet_split'] = [tweets.lower().split() for tweets in special_remove]\n",
    "    user_tweets['tweet_split'] = [filter(lambda x: not (x.startswith(\"@\") or x.startswith(\"#\") or x.startswith(\"https:\") or x in stopwords.words(\"english\") or x.startswith(\"rt\") or x[0].isdigit()), tweet) for tweet in user_tweets['tweet_split']]\n",
    "    user_tweets['tweet_split_string'] = [' '.join(str(x) for x in tweets) for tweets in user_tweets['tweet_split']]\n",
    "    return user_tweets\n",
    "\n",
    "#%%\n",
    "def comb_2(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the number of tweet combinations\n",
    "    \n",
    "    Argument: user_tweets\n",
    "    \n",
    "    Return: total number of tweet combinations\n",
    "    \"\"\"\n",
    "    num_tweets = len(user_tweets['text'])\n",
    "    return int(factorial(num_tweets) / (factorial(2) * factorial(num_tweets - 2)))\n",
    "\n",
    "#%%\n",
    "def sim_formula(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate set of pair in tweets\n",
    "    \n",
    "    Argument: user_tweets\n",
    "    \n",
    "    Return: set of pair in tweets ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        user_tweets = modify_df(user_tweets)\n",
    "        ind = [TextBlob(tweets).word_counts for tweets in user_tweets['tweet_split_string']]\n",
    "        vector_df = pd.DataFrame(ind)\n",
    "        vector_df = vector_df.fillna(0)\n",
    "        vector_matrix = vector_df.as_matrix()\n",
    "        idx = list(it.combinations(range(vector_df.shape[0]), 2))\n",
    "        sim_dot = {}\n",
    "        for i, j in idx:\n",
    "            sim_dot[(i, j)] = vector_matrix[i, :].dot(vector_matrix[j,:])\n",
    "\n",
    "        sim_norm = {}\n",
    "        for x, y in idx:\n",
    "            sim_norm[(x, y)] = LA.norm(vector_matrix[x, :]) * LA.norm(vector_matrix[y,:])\n",
    "        \n",
    "        dot_set = set(sim_dot)\n",
    "        norm_set = set(sim_norm)\n",
    "\n",
    "        sim_result = {}\n",
    "        for key in dot_set.intersection(norm_set):\n",
    "            if sim_norm[key] != 0:\n",
    "                sim_result[key] = (sim_dot[key] / sim_norm[key])/comb_2(user_tweets)\n",
    "        result_sum = sum(sim_result.values())\n",
    "    else:\n",
    "        result_sum = \"None\"\n",
    "\n",
    "    return result_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# #import similarity_2 as sim_2\n",
    "# from pandas.io.common import CParserError\n",
    "# import numpy as np\n",
    "\n",
    "# #%%\n",
    "# sample_data_id=pd.read_csv('tweets.csv')\n",
    "# #sample_data_id = pd.read_csv(open(\"tweets.csv\", 'rU'), encoding = 'utf-8', usecols = ['id'])\n",
    "# sample_data_id['user_id'] = sample_data_id['user_id'].astype(float)\n",
    "# sample_id = sample_data_id['user_id'].unique()\n",
    "\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\"\"\"\n",
    "verify tweet‘s authenticity using url_ratio, url_unique_ratio, hashtag_ratio, username_ratio and username_unique_ratio\n",
    "\"\"\"\n",
    "\n",
    "#from compiler.ast import flatten\n",
    "import re\n",
    "import collections\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n",
    "            \n",
    "#%%\n",
    "def url_ratio(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the percentage of 20 recent Tweets containing URLs\n",
    "    \n",
    "    Argument: tweets_df\n",
    "    \n",
    "    Return: tweets_url_ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        top_20 = user_tweets[:20]\n",
    "        tweets_url_ratio = sum(top_20['text'].str.contains(\"https:\") == True)/len(user_tweets['text'])\n",
    "    else:\n",
    "        tweets_url_ratio = 'None'\n",
    "    return tweets_url_ratio\n",
    "\n",
    "\n",
    "#%%\n",
    "def url_unique_ratio(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the ratio of the number of unique URLs in the 20 recent tweets\n",
    "    \n",
    "    Argument: tweets_df\n",
    "    \n",
    "    Return: url_ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        top_20 = user_tweets[:20]\n",
    "        # find all the urls using regular expression\n",
    "        urls = [re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(tweets)) for tweets in top_20['text']]\n",
    "        # flatten a list of lists\n",
    "        urls_flatten = flatten(urls)\n",
    "        #urls_flatten = urls.flatten()\n",
    "        # get the first two parts of the url\n",
    "        urls_split = [u.split('/')[0:3] for u in urls_flatten]\n",
    "        \n",
    "        urls_unique = [list(u) for u in set(tuple(u) for u in urls_split)]\n",
    "        url_unique= len(urls_unique)\n",
    "        tweet_total = len(user_tweets['text'])\n",
    "        url_ratio = url_unique/tweet_total\n",
    "    else:\n",
    "        url_ratio = 'None'\n",
    "    return url_ratio\n",
    "\n",
    "#%%\n",
    "def hashtag_ratio(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the hashtag ratio\n",
    "    \n",
    "    Argument: tweets_df\n",
    "    \n",
    "    Return: hashtag ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        top_20 = user_tweets[:20]\n",
    "        hashtag_ratio = sum(top_20['text'].str.contains(\"#\"))/len(top_20['text'])\n",
    "    else:\n",
    "        hashtag_ratio = 'None'\n",
    "    return hashtag_ratio\n",
    "\n",
    "#%%\n",
    "def username_ratio(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the username ratio\n",
    "    \n",
    "    Argument: tweets_df\n",
    "    \n",
    "    Return: username ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        top_20 = user_tweets[:20]\n",
    "        username_ratio = sum(top_20['text'].str.contains(\"@\"))/len(top_20['text'])\n",
    "    else:\n",
    "        username_ratio = 'None'\n",
    "    return username_ratio\n",
    "    \n",
    "#%%\n",
    "def username_unique_ratio(user_tweets):\n",
    "    \"\"\"\n",
    "    calculate the ratio of the number of unique @usernames\n",
    "    \n",
    "    Argument: tweets_df\n",
    "    \n",
    "    Return: username_unique_ratio\n",
    "    \"\"\"\n",
    "    if len(user_tweets) != 0: \n",
    "        top_20 = user_tweets[:20]\n",
    "    \n",
    "        username = [re.findall('@([A-Za-z0-9_]+)', str(tweets)) for tweets in top_20['text']]\n",
    "        # flatten a list of lists\n",
    "        username_flatten = flatten(username)\n",
    "        #username_flatten = username.flatten()\n",
    "        username_unique = set(username_flatten)\n",
    "        user_unique= len(username_unique)\n",
    "        # total number of users that were being @, not all the tweets\n",
    "        tweet_total = len(user_tweets['text'])\n",
    "        user_ratio = user_unique/tweet_total\n",
    "    else:\n",
    "        user_ratio = 'None'\n",
    "    return user_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d453a39dd4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mt_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_data_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_data_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msimilarity_ratio_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msim_formula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0murl_ratio_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a03eb7a4c803>\u001b[0m in \u001b[0;36msim_formula\u001b[0;34m(user_tweets)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdot_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msim_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0msim_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msim_dot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msim_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcomb_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mresult_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a03eb7a4c803>\u001b[0m in \u001b[0;36mcomb_2\u001b[0;34m(user_tweets)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m     \u001b[0mnum_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tweets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "files=['../data/datasets_full.csv/genuine_accounts.csv/tweets.csv',\n",
    "       '../data/datasets_full.csv/social_spambots_1.csv/tweets.csv',\n",
    "       '../data/datasets_full.csv/social_spambots_2.csv/tweets.csv',\n",
    "       '../data/datasets_full.csv/social_spambots_3.csv/tweets.csv',\n",
    "       '../data/datasets_full.csv/traditional_spambots_1.csv/tweets.csv',\n",
    "       '../data/datasets_full.csv/fake_followers.csv/tweets.csv']\n",
    "\n",
    "i=0\n",
    "sample_tweet_analysis_ratio = pd.DataFrame(columns = [\"id\", \"similarity_ratio_2\",\"url_ratio\", \"url_unique_ratio\", \"hashtag_ratio\", \"username_ratio\", \"username_unique_ratio\"])\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    #path=  '⁨Users/harinath⁩/Downloads⁩/'+file\n",
    "    sample_data_id=pd.read_csv(file)\n",
    "    #sample_data_id = pd.read_csv(open(\"tweets.csv\", 'rU'), encoding = 'utf-8', usecols = ['id'])\n",
    "    sample_data_id['user_id'] = sample_data_id['user_id'].astype(float)\n",
    "    sample_id = sample_data_id['user_id'].unique()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    for user in sample_id:\n",
    "        t_df=sample_data_id.loc[sample_data_id['user_id'] == user]\n",
    "\n",
    "        similarity_ratio_2=sim_formula(t_df)\n",
    "\n",
    "        url_ratio_out = url_ratio(t_df)\n",
    "\n",
    "        #%%\n",
    "        url_unique_ratio_out = url_unique_ratio(t_df)\n",
    "\n",
    "        #%%\n",
    "        hashtag_ratio_out = hashtag_ratio(t_df)\n",
    "\n",
    "        #%%\n",
    "        username_ratio_out = username_ratio(t_df)\n",
    "\n",
    "        #%%\n",
    "        username_unique_ratio_out = username_unique_ratio(t_df)\n",
    "        #similarity_ratio_2 = [sim_formula(sample_tw[int(i)]) for i in range(len(sample_tw))]\n",
    "        sample_tweet_analysis_ratio.loc[i] = [user,similarity_ratio_2,url_ratio_out, url_unique_ratio_out, hashtag_ratio_out, username_ratio_out, username_unique_ratio_out]\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge these features with original table\n",
    "data_org=pd.read_csv(data_no_additional_feature)\n",
    "new_df = pd.merge(data_org, sample_tweet_analysis_ratio,  how='inner', left_on=['id',], right_on = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = new_df.loc[:,'label']\n",
    "trainX=new_df[new_df.columns.difference(['label','id', 'Unnamed: 0', 'name', 'screen_name',\n",
    "       'url', 'lang', 'time_zone', 'location', 'default_profile',\n",
    "       'default_profile_image', 'geo_enabled', 'profile_image_url',\n",
    "       'profile_banner_url', 'profile_use_background_image',\n",
    "       'profile_background_image_url_https', 'profile_text_color',\n",
    "       'profile_image_url_https', 'profile_sidebar_border_color',\n",
    "       'profile_background_tile', 'profile_sidebar_fill_color',\n",
    "       'profile_background_image_url', 'profile_background_color',\n",
    "       'profile_link_color', 'utc_offset', 'protected', 'verified',\n",
    "       'description', 'created_at', 'updated', 'file'])]\n",
    "#trainY=trainY.fillna(trainY.median())\n",
    "\n",
    "\n",
    "\n",
    "trainX=np.array(trainX)\n",
    "trainY =np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c506b081153c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#statuses count\n",
    "#ff_ratio\n",
    "#tweet rate\n",
    "rem_features=['label','id', 'Unnamed: 0', 'name', 'screen_name',\n",
    "       'url', 'lang', 'time_zone', 'location', 'default_profile',\n",
    "       'default_profile_image', 'geo_enabled', 'profile_image_url',\n",
    "       'profile_banner_url', 'profile_use_background_image',\n",
    "       'profile_background_image_url_https', 'profile_text_color',\n",
    "       'profile_image_url_https', 'profile_sidebar_border_color',\n",
    "       'profile_background_tile', 'profile_sidebar_fill_color',\n",
    "       'profile_background_image_url', 'profile_background_color',\n",
    "       'profile_link_color', 'utc_offset', 'protected', 'verified',\n",
    "       'description', 'created_at', 'updated', 'file']\n",
    "\n",
    "cols=['Features']\n",
    "feature_importances=pd.DataFrame(columns=cols)\n",
    "feature_importances['Features'] = new_df[new_df.columns.difference(rem_features)].columns\n",
    "train_auc=[]\n",
    "test_auc=[]\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "i = 0\n",
    "for train, test in cv.split(trainX, trainY):\n",
    "    # The \"accuracy\" scoring is proportional to the number of correct\n",
    "    # classifications\n",
    "    classifier = xgb.XGBClassifier(objective='binary:logistic' , booster='gbtree', eta=0.001,\n",
    "                               max_depth=20,colsample_bytree=0.7, subsample=0.8,min_child_weight=5,gamma=0,\n",
    "                                n_estimators=100, \n",
    "                                n_jobs=-1, verbose=False)\n",
    "    #classifier=xgb.XGBClassifier(**params)\n",
    "    classifier.fit(trainX[train], trainY[train],\n",
    "                        eval_set=[(trainX[train], trainY[train]), \n",
    "                                  (trainX[test], trainY[test])],\n",
    "                                eval_metric='logloss', verbose=False)\n",
    "    \n",
    "    evals_result= classifier.evals_result()\n",
    "    text='Value_'+str(i)\n",
    "    i=i+1\n",
    "    feature_importances[text] = classifier.feature_importances_\n",
    "\n",
    "    train_auc.append(evals_result['validation_0']['logloss'][99])\n",
    "    test_auc.append(evals_result['validation_1']['logloss'][99])\n",
    "    #print(evals_result)\n",
    "    \n",
    "\n",
    "predict = classifier.predict(trainX[test])\n",
    "print(classification_report(trainY[test], predict))\n",
    "    \n",
    "mean_auc_train = np.mean(train_auc)\n",
    "std_auc_train = 1.96*np.std(train_auc)\n",
    "# \n",
    "mean_auc_test = np.mean(test_auc)\n",
    "std_auc_test = 1.96*np.std(test_auc)\n",
    "# \n",
    "print('Test set auc:' + str(mean_auc_train) + '(CI :'+ str(std_auc_train) + ')')\n",
    "print('Train set auc:' + str(mean_auc_test) + '(CI :'+ str(std_auc_test) + ')')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
